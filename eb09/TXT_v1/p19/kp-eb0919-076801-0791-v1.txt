+=====================================================================+
 ENCYCLOPEDIA BRITANNICA, NINTH EDITION: A MACHINE-READABLE TEXT
 TRANSCRIPTION, v1.0, The Nineteenth-Century Knowledge Project, 2022.
 nckp@temple.edu, https://tu-plogan.github.io/

 Source: Encyclopaedia Britannica: A Dictionary of Arts, Sciences,
 and General Literature. 9th ed., 25 vols. NY: Charles Scribner's
 Sons, 1875-1889. Image scans: California Digital Archive.

 License: This work is licensed under the Creative Commons Attribution
 4.0 International License. To view a copy of this license, visit
 http://creativecommons.org/licenses/by/4.0/ or send a letter to
 Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

 This entry: 9th edition, volume 19, page 768 [9:19:768]
+=====================================================================+


PROBABILITY T

HE mathematical theory of probability is a science which aims at reducing to calculation, where possible, the amount of credence due to propositions or statements, or to the occurrence of events, future or past, more especially as contingent or dependent upon other propositions or events the probability of which is known.

Any statement or (supposed) fact commands a certain amount of credence, varying from zero, which means conviction of its falsity, to absolute certainty, denoted by unity. An even chance, or the probability of an event which is as likely as not to happen, is represented by the fraction ⅜. It is to be observed that ∣ will be the probability of an event about which we have no knowledge whatever, because if we can see that it is more likely to happen than not, or less likely than not, we must be in possession of some information respecting it. It has been proposed to form a sort of thermometrical scale, to which to refer the strength of the conviction we have in any given case. Thus if the twenty-six letters of the alphabet have been shaken together in a bag, and one letter be drawn, we feel a very feeble expectation that A has been the one taken. If two letters be drawn, we have still very little confidence that A is one of them; if three be drawn, it is somewhat stronger; and so on, till at last, if twenty-six be drawn, we are certain of the event, that is, of A having been taken.

Probability, which necessarily implies uncertainty, is a consequence of our ignorance. To an omniscient Being there can be none. Why, for instance, if we throw up a shilling, are we uncertain whether it will turn up head or tail? Because the shilling passes, in the interval, through a series of states which our knowledge is unable to predict or to follow. If we knew the exact position and state of motion of the coin as it leaves our hand, the exact value of the final impulse it receives, the laws of its motion as affected by the resistance of the air and gravity, and finally the nature of the ground at the exact spot where it falls, and the laws regulating the collision between the two substances, we could predict as certainly the result of the toss as we can which letter of the alphabet will be drawn after twenty-five have been taken and examined.

The probability, or amount of conviction accorded to any fact or statement, is thus essentially subjective, and varies with the degree of knowledge of the mind to which the fact is presented (it is often indeed also influenced by passion and prejudice, which act powerfully in warping the judgment),—so that, as Laplace observes, it is affected partly by our ignorance partly by our knowledge. Thus, if the question were put, Is lead heavier than silver? some persons would think it is, but would not be surprised if they were wrong; others would say it is lighter; while to a worker in metals probability would be superseded by certainty. Again, to take Laplace’s illustration, there are three urns A, B, C, one of which contains black balls, the other two white balls; a ball is drawn from the urn C, and we want to know the probability that it shall be black. If we do not know which of the urns contains the black balls, there is only one favourable chance out of three, and the probability is said to be ⅜. But if a person knows that the urn A contains white balls, to him the uncertainty is confined to the urns B and C, and therefore the probability of the same event is ∣. Finally to one who had found that A and B both contained white balls, the probability is converted into certainty.

In common language, an event is usually said to be likely or probable if it is more likely to happen than not, or when, in mathematical language, its probability exceeds ⅜; and it is said to be improbable or unlikely when its probability is less than j. Not that this sense is always adhered to; for, in such a phrase as “It is likely to thunder to-day,” we do not mean that is more likely than not, but that in our opinion the chance of thunder is greater than usual; again, “Such a horse is likely to win the Derby,” simply means that he has the best chance, though according to the betting that chance may be only ⅜. Such unsteady and elliptical employment of words has of course to be abandoned and replaced by strict definition, at least mentally, when they are made the subjects of mathematical analysis. Certainty, or absolute conviction, also, as generally understood, is different from the mathematical sense of the word certainty. It is very difficult and often impossible, as is pointed out in the celebrated Grammar of Assent, to draw out the grounds on which the human mind in each case yields that conviction, or assent, which, according to Newman, admits of no degrees, and either is entire or is not at all.^[1. “There is a sort of leap which most men make from a high probability to absolute assurance . . . analogous to the sudden consilience, or springing into one, Of the two images seen by binocular vision, when gradually brought within a certain proximity. ”—Sir J. Herschel, in Edin. Review, July 1850.] If, when walking on the beach, we find the letters “Constantinople” traced on the sand, we should feel, not a strong impression, but absolute certainty, that they were characters not drawn at random, but by one acquainted with the word so spelt. Again, we are certain of our own death as a future event; we are certain, too, that Great Britain is an island; yet in all such cases it would be very difficult, even for a practised intellect, to present in logical form the evidence, which nevertheless has compelled the mind in each instance to concede the point.^[2. Archbishop Whately’s jeu d , esprit, Historic Doubts respecting Napoleon Bonaparte, is a good illustration of the difficulties there may be in proving a conclusion the certainty of which is absolute.] Mathematical certainty, which means that the contrary proposition is inconceivable, is thus different, though not perhaps as regards the force of the mental conviction, from moral or practical certainty. It is questionable whether the former kind of certainty is not entirely hypothetical, and whether it is ever attainable in any of the affairs or events of the real world around us. The truth of no conclusion can rise above that of the premises, of no theorem above that of the data. That two and two make four is an incontrovertible truth; but before applying even it to a concrete instance we have to be assured that there were really two in each constituent group; and we can hardly have mathematical certainty of this, as the strange freaks of memory, the tricks of conjurors, &c., have often made apparent.

There is no more remarkable feature in the mathematical theory of probability than the manner in which it has been found to harmonize with, and justify, the conclusions to which mankind have been led, not by reasoning, but by instinct and experience, both of the individual and of the race. At the same time it has corrected, extended, and invested them with a definiteness and precision of which these crude, though sound, appreciations of common sense were till then devoid. Even in cases where the theoretical result appears to differ from the common-sense view, it often happens that the latter may, though perhaps unknown to the mind itself, have taken account of circumstances in the case omitted in the data of the [9:19:769]

theoretical problem. Thus, it may be that a person accords a lower degree of credence to a fact attested by two or more independent witnesses than theory warrants,—the reason being that he has unconsciously recognized the possibility of collusion, which had not been presented among the data. Again, it appears from the rules for the credibility of testimony that the probability of a fact may be diminished by being attested by a new witness, viz., in the case where his credibility is less than ∣. This is certainly at variance with our natural impression, which is that our previous conviction of any fact is clearly not weakened, however little it be intensified, by any fresh evidence, however suspicious, as to its truth. But on reflexion we see that it is a practical absurdity to suppose the credibility of any witness less than ∣—that is, that he speaks falsehood oftener than truth—for all men tell the truth probably nine times out of ten, and only deviate from it when their passions or interests are concerned. Even where his interests are at stake, no man has any preference for a lie, as such, above the truth; so that his testimony to a fact will at worst leave the antecedent probability exactly what it was.

A celebrated instance of the confirmation and completion by theory of the ordinary view is afforded by what is known as James Bernoulli’s theorem. If we know the odds in favour of an event to be three to two, as for instance that of drawing a white ball from a bag containing three white and two black, we should certainly judge that if we make five trials we are more likely to draw white three times and black twice than any other combination. Still, however, we should feel that this was very uncertain; instead of three white, we might draw white 0, 1, 2, 4, or 5 times. But if we make say one thousand trials, we should feel confident that, although the numbers of white and black might not be in the proportion of three to two, they would be very nearly in that proportion. And the more the trials are multiplied the more closely would this proportion be found to obtain. This is the principle upon which we are continually judging of the possibility of events from what is observed in a certain number of cases.^[3. So it is said, “the tree is known by its fruits”; “practice is better than theory”; and the universal sense of mankind judges that the safest test of any new invention, system, or institution is to see how it works. So little are we able by a priori speculations to forecast the thousand obstacles and disturbing influences which manifest themselves when any new cause or agent is introduced as a factor in the world’s affairs.] Thus if, out of ten particular infants, six are found to live to the age of twenty, we judge, but with a very low amount of conviction, that nearly six-ten ths of the whole number born live to twenty. But if, out of 1,000,000 cases, we find that 600,000 live to be twenty, we should feel certain that the same proportion would be found to hold almost exactly were it possible to test the whole number of cases, say in England during the 19th century. In fact we may say, considering how seldom we know a priori the probability of any event, that the knowledge we have of such probability in any case is entirely derived from this principle, viz., that the proportion which holds in a large number of trials will be found to hold in the total number, even when this may be infinite, —the deviation or error being less and less as the trials are multiplied.

Such no doubt is the verdict of the common sense of mankind, and it is not easy to say upon what considerations it is based, if it be not the effect of the unconscious habit which all men acquire of weighing chances and probabilities, in the state of ignorance and uncertainty which human life is. It is now extremely interesting to see the results of the unerring methods of mathematical analysis when applied to the same problem. It is a very difficult one, and James Bernoulli tells us he reflected upon it for twenty years. His methods, extended by De Moivre and Laplace, fully confirm the conclusions of rough common sense; but they have done much more. They enable us to estimate exactly how far we can rely on the proportion of cases in a large number of trials, truly representing the proportion out of the total number—that is, the real probability of the event. Thus he proves that if, as in the case above mentioned, the real probability of an event is ∣, the odds are 1000 to 1 that, in 25,550 trials, the event shall occur not more than 15,841 timesand not less than 14,819 times,—that is, that the deviation from 15,330, or ⅛ of the whole, shall not exceed - g l τy of the whole number of trials.

The history of the theory of probability, from the celebrated question as to the equitable division of the stakes between two players on their game being interrupted, proposed to Pascal by the Chevalier de Méré in 1654, embracing, as it does, contributions from almost all the great names of Europe during the period, down to Laplace and Poisson, is elaborately and admirably given by Mr Todhunter in his History of the subject, now a classical work. It was not indeed to be anticipated that a new science which took its rise in games of chance, and which had long to encounter an obloquy, hardly yet extinct, due to the prevailing idea that its only end was to facilitate and encourage the calculations of gamblers, could ever have attained its present status—that its aid should be called for in every department of natural science, both to assist in discovery, which it has repeatedly done (even in pure mathematics), to minimize the unavoidable errors of observation, and to detect the presence of causes as revealed by observed events. Nor are commercial and other practical interests of life less indebted to it:^[4. Men were surprised to hear that not only births, deaths, and marriages, but the decisions of tribunals, the results of popular elections, the influence of punishments in checking crime, the comparative values of medical remedies, the probable limits of error in numerical results in every department of physical inquiry, the detection of causes, physical, social, and moral, nay, even the weight of evidence and the validity of logical argument, might come to be surveyed with the lynx-eyed scrutiny of a dispassionate analysis.— Sir J. Herschel.] wherever the future has to be forecasted, risk to be provided against, or the true lessons to be deduced from statistics, it corrects for us the rough conjectures of common sense, and decides which course is really, according to the lights of which we are in possession, the wisest for us to pursue. It is sui generis and unique as an application of mathematics, the only one, apparently, lying quite outside the field of physical science. De Moivre has remarked that, “some of the problems about chance having a great appearance of simplicity, the mind is easily drawn into a belief that their solution may be attained by the mere strength of natural good sense”; and it is with surprise we find that they involve in many cases the most subtle and difficult mathematical questions. It has been found to tax to the utmost the resources of analysis and the powers of invention of those who · have had to deal with the new cases and combinations which it has presented. Great, however, as are the strictly mathematical difficulties, they cannot be said to be the principal. Especially in the practical applications, to detach the problem from its surroundings in rerum natura, discarding what is non-essential, rightly to estimate the extent of our knowledge respecting it, neither tacitly assuming as known what is not known, nor tacitly overlooking some datum, perhaps from its very obviousness, to make sure that events we are taking as independent are not really connected, or probably so,—such are the preliminaries necessary before the question is put in the scientific form to which calculation can be applied, and failing which the [9:19:770] result of the mathematician will be but an ignoratio elenchi —a correct answer, but to a different question.

From its earliest beginnings, a notable feature in our subject has been the strange and insidious manner in which errors creep in—often misleading the most acute minds, as in the case of D’Alembert—and the difficulty of detecting them, even when one is assured of their presence by the evident incorrectness of the result. This is probably in many cases occasioned by the poverty of language obliging us to use one term in the same context for different things—thus introducing the fallacy of ambiguous middle; e.g., the same word “probability” referring to the same event may sometimes mean its probability before a certain occurrence, sometimes after ; thus the chance of a horse winning the Derby is different after the Two Thousand from what it was before. Again, it may mean the probability of the event according to one source of information, as distinguished from its probability taking everything into account ; for instance, an astronomer thinks he can notice in a newly-discovered planet a rotation from east to west; the probability that this is the case is of course that of his observations in like cases turning out correct, if we had no other source of information; but the actual probability is less, because we know that at least the vast majority of the planets and satellites revolve from west to east. It is easy to see that such employment of terms in the same context must prove a fruitful source of fallacies; and yet, without wearisome repetitions, it cannot always be avoided. But, apart from mere logical errors, the main stumbling-block is no doubt the uncertainty as to the limits of our knowledge in each case, or—though this may seem a contradiction in terms— the difficulty of knowing what we do know; and we certainly err as often in forgetting or ignoring what we do know, as in assuming what we do not. It is a not uncommon popular delusion to suppose that if a coin has turned up head, say five times running, or the red has won five times at roulette, the same event is likely to occur a sixth time; and it arises from overlooking (perhaps from the imagination being struck by the singularity of the occurrence) the a priori knowledge we possess, that the chance at any trial is an even one (supposing all perfectly fair); the mind thus unconsciously regards the event simply as one that has recurred five times, and therefore judges, correctly, that it is very likely to occur once more. Thus if we are given a bag containing a number of balls, and we proceed to draw them one by one, and the first five drawn are white, the odds are 6 to 1 that the next will be white,—the slight'information afforded by the five trials being thus of great importance, and strongly influencing the probabilities of the future, when it is all we have to guide us, but absolutely valueless, and without influence on the future, when we have a priori certain information. The lightest air will move a ship which is adrift, but has simply no effect on one securely moored.

It is not to be supposed that the results arrived at when the calculus of probabilities is applied to most practical questions are anything more than approximations; but the same may be said of almost all such applications of abstract science. Partly from ignorance of the real state of the case, partly from the extreme intricacy of the calculations requisite if all the conditions which we do or might know are introduced, we are obliged to substitute in fact, for the actual problem, a simpler one approximately representing it. Thus, in mechanical questions, assumptions such as that the centre of gravity of an actual sphere is at its centre, that the friction of the rails on a railway is constant at different spots or at different times, or that in the rolling of a heavy body no depression is produced by its weight in the supporting substance, are instances of the convenient fictions which simplify the real question, while they prevent us accepting the result as more than something near the truth. So in probability, the chance of life of an individual is taken from the general tables (unless reasons to the contrary are very palpable) although, if his past history, his mode of life, the longevity of his family, àc., were duly weighed, the general value ought to be modified in his case; again, in attempting to estimate the value of the verdict of a jury, whether unanimous or by a majority, each man is supposed to give his honest opinion,—feeling and prejudice, or pressure from his fellow-jurors, being left out of' the account. Again, the value of an expectation to an individual is taken to be measured by the sum divided by his present fortune, though it is clearly affected by other circumstances, as the number of his family, the nature of his business, &c. An event has been found to occur on an average once a year during a long period: it is not difficult to show that the chance of its happening in a particular year is 1 - e^ 1 , or 2 to 1 nearly. But, on examining the record, we observe it has never failed to occur during three years running. This fact increases the above chance; but to introduce it into the calculation at once renders the question a very difficult one. Even in games of chance we are obliged to judge of the relative skill of two players by the result of a few games; now one may not have been in his usual health, &c., or may have designedly not played his best; when he did win he may have done so by superior play, or rather by good luck; again, even in so simple a case as pitch and toss, the coin may, in the concrete, not be quite symmetrical, and the odds of head or tail not quite even.

Not much has been added to our subject since the close of Laplace’s career. The history of science records more than one parallel to this abatement of activity. When such a genius has departed, the field of his labours seems exhausted for the time, and little left to be gleaned by his successors. It is to be regretted that so little remains to us of the inner working of such gifted minds, and of the clue by which each of their discoveries was reached. The didactic and synthetic form in which these are presented to the world retains but faint traces of the skilful inductions, the keen and delicate perception of fitness and analogy, and the power of imagination—though such a term may possibly excite a smile when applied to such dry subjects—which have doubtless guided such a master as Laplace or Newton in shaping out each great design— only the minor details of which have remained over, to be supplied by the less cunning hand of commentator and disciple.

We proceed to enumerate the principal divisions of the theory of probability and its applications. Under each we will endeavour to give at least one or two of the more remarkable and suggestive questions which belong to it,— especially such as admit of simplification or improvement in the received solutions; in such an article as the present we are debarred from attempting even an outline of the whole. We will suppose the general fundamental principles to be already known to the reader, as they are to be now found in several elementary works, such as Todhunter’s Algebra, Whitworth’s Choice and Chance, <tc.

Many of the most important results are given under the apparently trifling form of the chances in drawing balls from an urn, <fcc., or seem to relate to games of chance, as dice or cards, but are in reality of far wider application,— this form being adopted as the most definite and lucid manner of presenting the chances of events occurring under circumstances which may be assimilated, more or less closely, to such cases.
[9:19:771]

I. Determination of the Probabilities of Compound Events, when the Probabilities of the Simple Events on which THEY DEPEND ARE KNOWN.

1. Under this head come a very large and diversified range of questions; a very few of the most important are all that we can give. One great class relates to the fulfilment of given conditions in repeated trials as to the same event, knowing the probability of what will happen in each trial.

2. Let there be an event which must turn out in one of two ways, W and B (as in drawing a ball from an urn containing white and black balls only); let the respective probabilities for each trial be p, q ; so that p + q=Λ. Lettwo trials be made: the four possible cases which may arise are

WW, WB, BW, BB.

The probability of the first is p 2 , of the second pq, of the third pq, of the fourth q 2 . Thus the probability of a white and a black ball being drawn in an assigned order is pq ; but that of a white and a black in any order is 2pq.

Suppose now n trials to be made. The probability of W every time is p n ; that of B once and W (n -1) times in an assigned order is p n ~ 1 q, but if the order is indifferent it is np n ~ 1 q∙, that of B occurring twice only is p n ~ 2 <f∙ if the order is given, but

~p"~ 2 g 8 in any order; and so on. We have then this Δ result :—in the binomial expansion {p + ?)” “1 =ψ n + n P n ^ ⅛ + ” j 2 ^¾ l1 ^ 2 2 2 + *rS⅛=V-⅜÷^ (i).

the terms in their order give the probabilities of the event W happening n times; ofW (n-l) times and B once; ofW (n-2) times and B twice; and so on,—the sum of the whole giving 1, that is, certainty.

3. As an example, let A and B be two players whose respective chances of winning one game are p and q ; to find the probability of A winning m games before B wins n games, the play terminating when either of these events has occurred.

The chance of A winning the first m games is p m . The chance of his winning in the first wι + l games is mp m ~ 1 q. p=mp m q; for he must have won m -1 games out of the first m, and then win the (™ + l)th; otherwise we should be including the first case. Again, the chance of A winning in the first wι + 2 games is, in like (τn + l)τn „ 1 9 (m + l)τn m „ , ι , t ., manner, - - p m ~ γ q 2 p =—ττ~P i 2 > anc ^ so on ∙ -^ ow the

Z Δ

match must be decided at latest by the (m + n-l)fh game; for, if A fails to win m games by that time, B must have won n. Hence the chance of A winning the match is f τn(∞ + l) m(m + l) . . . (wι + n-2) „ 1 )

?”*11 + + ∣2 9 + - · [ to - 1 ⅛"' 1 j∙∙

Thus, if A’s skill be double that of B, the chance that A wins 112 131

four games before B wins two is · That of B winning is .

If A and B agree to leave off playing before the match is decided, the stakes ought clearly to be divided between them in proportion to their respective probabilities of winning, as given above,—putting for m and n the numbers of games required to be won, at any given point of the match, by A and B respectively.

This was one of the questions proposed to Pascal by the Chevalier de Mere in the year 1654.

4. In the expansion (1) it may be asked which combination of the events W, B is most likely to occur in the n trials. As the ratio of the 2d term to the 1st is n -, of the 3d to the 2d n ~ - -, p 2 p' and of the (r + l)th to the rth -—so long as this ratio τ p continues to increase the terms will increase. The condition, therefore, for the rth teπn to be the greatest is n-r+1 ^p . ,.

—~ r — <f; or ’> («+l)s r i that is, r is the next integer above (n + l)g ’.

We conclude that if r is the next integer below (n + l)g ’ the (r + l)th term is.the greatest—that is, it is most likely that the event Μ occurs n — r times and B r times. If (n + V)q should be an integer (r), B is as likely to occur r as r + 1 times; and either is more probable than any other number. Thus, in twelve throws of a die, the ace is more likely to turn up twice than any other number; while in eleven throws it is as likely to turn up once only as twice.

It is important to remark that, if the number of trials n be very large, we may treat qn and pn as whole numbers, and conclude that the event W is more likely to happen pn times and B qn times than in any other proportion.

5. Among the many questions which relate to the occurrence of different combinations in successive trials as to the same event, one is as to the chances for a succession, or run, of the same result several times.

Let us consider the very simple case—In n throws of a coin, what is the chance that head occurs (at least) twice running?

This will be an instance of the aid afforded by the calculus of finite differences in questions on probability. Let w r = the number of cases of r throws of a coin in which head turns up twice running, the whole number of cases being of course 2 r . Now if we consider the value of ‰ +3 , it includes 2u n +2, because the (n + 3)th throw may turn up two ways; but it includes also those cases when head turns up in the last two throws, tail in the preceding one, and no run of two heads occurs in the n preceding ones. The number of these cases is 2 n - ω n ∙ We have therefore the equation

‰ + 3=2‰ +2 + 2 n -u n (2).

If E be an operator such that Ew r = u r+1 , equation (2) is (E 3 -2E 2 + l)∙u n =2 n ; or, (E - 1)(E 2 - E - l)n, 1 = 2»; so that, if we put a, β for the roots of the equation E 2 - E -1 = 0, u n =2 n + A + Bα n + C3 n (3), since u n ≈2 n is a particular solution of (2),—A, B, C being three undetermined constants.

Now in two throws there is one case where head turns up twice, and in three throws there are three cases; hence we have Uγ = 0 = 2 + A + Bα + Cj3 n 2 = 1 = 4 + A + Ba 2 + Cj8 2 n 3 = 3 = 8 +A + Ba 3 + C3 3 ; and, remembering that α 2 =α + l, 0 2 = j3 + l, we shall easily find from these

A=0, B=-^-, C≈- o ; ’ β-a ’ a-β a n+2 _ fln⅛2

so that n n = 2 n £— (4).

a-β

XT l + √5 o 1 "√ 5

Now a 2 ’^ = 2 j expanding by the binomial theorem and reducing, «.-* -g⅛∣ι + ⅛∣⅛ lι+ ⅛÷W>-W*-⅜ 4... J..(5 ) i dividing by the total number of cases 2 n , we have for the probability of head turning up at least twice running in n throws n ι » + 2 (1 , (w + l)n- (n + l)n(π-l)(n-2) 2 1

2‰≈ 1 -2≡i+i ∣ 1 +-ηy-5+ £ 52 + . . . j .. (6).

Another method of obtaining the same result is to consider the number of cases in which head never occurs twice running; let u n be this number, then 2 n -u n must be the number of cases when head occurs at least twice successively. Consider the value of u n + 2 ∙, if the last or(n + 2)th throw be tail, n π+2 includes all the eases (‰+ 1 ) of the n +1 preceding throws which gave no succession of heads; and if the last be head the last but one must be tail, and these two may be preceded by any one of the u n favourable cases for the first n throws. Consequently

‰+2 = ‰+1 + ‰.

If α, β, as before, are the roots of the quadratic E 2 - E -1 = 0, this equation gives n n =Aα" + B8".

Here A and B are easily found from the conditions n 1 = 2, m 2 =3; viz., a-⅛b-Λ- whence u n = ∙jl + γ- 2 .' 5 + &c. j∙

as in eq. (5). The probability that head never turns up twice running is found by dividing this by 2”, the whole number of cases. This probability of course becomes smaller and smaller as the number of trials (ri) is increased.

6. Let us consider the chance of a run of three heads or tails during n throws,—that of a run of two heads or tails being evidently—” ~- 2 = 1 - -} 1 , as there are but two cases out of the z z

2“ which are alternately head and tail.

Let u r be the number of cases, during r throws, which give at least one succession of three heads or three tails. Consider the value of ‰ +3 ; it includes 2‰ +2 , as the last throw may be head or tail; but, besides these, every case of the first n throws which contains no run of three gives rise to one new case of the n + 3 having a run of three; thus, if the nth throw be head, the last four may be HTTT, or THHH if the nth be tail. Hence

‰ +3 =2w n+2 + 2 n -w n , [9:19:772] the same equation of differences as (2). Its solution is equation (3), in which, if we determine the constants by the conditions w 1 = 0, u i = 0, h 3 = 2, and divide by 2 n , we find for the probability of a run of three of either event during n trials u n , n + 1 f, ι π(n-l) κ , n(n-l)(π-2)(n-3) κs 1 ) , λ

-2i-l-2*rψ + -13- 5 + [5 5 +..∙J7).

Comparing this result with (6) we find that the chance of a run of two heads in n trials is equal to the chance of a run of three, of either heads or tails, in n +1 trials.

7. If an event may turn out on each trial in a + b ways, of which a are favourable and b unfavourable (thus a card may be drawn from a pack in fifty-two ways, twelve of which give court cards), and if we consider the probability that during τι trials there shall occur a run of at least p favourable results, it is not difficult to see that (w r denoting the number of ways this may occur in r trials)

m m+p+ i - (α + b)u n+v + ba r {(a+ b) n - u n }, as u n ⅛+ 1 includes, besides (a + 5)u n+p , those cases in which the last p trials are favourable, the one before unfavourable, and the n preceding containing no such run as stated.

We will not enter on Laplace’s solution of this equation, or rather of one equivalent to it, especially as the result is not a simple one (see Todhunter, p. 185).

8. Let the probability of an event happening in one trial be p, that of its failing q ; we have seen (art. 4) that, if a large number N of trials be made, the event is most likely to happen pN times and fail <?N times. The chance of this occurring is, however, extremely small, though greater than that in favour of any other proportion. We propose now to examine the probability that the proportion ol successes shall not deviate from its most probable value by more than a given limit—that is, in fact, to find the probability that in N trials the number of times in which the event happens shall lie between the two limits pN±r.

Let τn=pN, 7ι=3N, which are taken tobe integers. The probability of the event happening m times is the greatest term T of the expansion (1), viz.,

IN

T=i-=r-2> m i" · I to |_n a

The calculation of this would be impracticable when N, to, n are large numbers, but Stirling’s theorem gives us

1.2.3 . . . x = z*+ie- a '√2ir, very nearly, when x is large; and by substituting in the preceding value of T, and reducing, we easily find

T= 1 (8).

√2πpjN k '

Now the terms of the expansion (1) on either side of T are

_ n(n—1) j g* τ n p m q m ( fft -l) 9 ¾ . .

(m-HXm+2) qt m-f-1 √ ++ n⅛l i > (n+iχn+2),p2 + ,

But if x is much greater than a, x -a≈xe~χ nearly, so that n(n - l)(n-2). . . (s terms) = ¾⅛- 1 +- + ^ ' ( * *'-¾*e~⅞p; a ls° (to + 1)(to + 2). . . (s terms)=TO ’ t-⅛^ ’

Hence the sth term before T in (9) is

( n ∖∙ φ-l) *(*÷1) ∕ » ∖∙ Nj 2 1 m—n

∖TO∕ 2 " β 2m ∖J∕ , ° rβ 2mn + 2mn i T.

The sth term after T is

N< 2 _m—n e 2mn 2mn*T.

Now the probability that the event shall happen a number of times comprised between m + r and m-r is the sum of the terms in (9) f∣∙om the rth term before T to the rth term after T. (N. B., though r may be large, it is supposed small as compared with N, to, or n.)

Now the sth term before T+the sth term after T = 2e~^T, since e x + e~ x -2, when x≈~^s is small. Taking then each term before T with the corresponding term after T, and putting for shortness r °

2 N 1

α 2toto - 2j¾N ^θ)∙

we have for the required probability

A∙ — 2(⅜T + Te~ α, +Te~ 22β *+Te~ 82α2 + .... Te- , * a2 ).

If we now consider the curve whose equation is y=Te~ ir2 , and take a series of its ordinates corresponding to ar—0, a, 2a, 3α . . . . r«, where a is very small,, and if A be its area from x=0 to x=ra, then

A

- = ⅛(first+ last ordinates) + sum of intermediate ordinates a 2 . ’ . p r = - A + last ordinate, α

= - T Γ∖ - χ2 dx + Te~ r2a2 a yo or Γr = -7 - ∕* ,a e -x ⅛ + - 1 c ~ ’ ' 2a2 (11).

√√0 √2p i Nιr

9. We refer to the integral calculus for the methods of computing the celebrated integral J^e~ χ2 dx, and will give here a short table of its values.

Table of the Values of the Integral I—-7- f τ e~ χ2 dx. ∖J ιr∕0
[table]

If the value of I is 0 ’ 5, or ⅜, τ= , 4769.

10. The second term in formula (11) expresses the probability that the number of occurrences of the event shall be exactly m + r or m-r, or more correctly the mean of these two probabilities. It may be neglected when the number of trials N is very great and the deviation r not a very small number.

We see from the foregoing table that when rα =— r —3

√2∕^N

it becomes practically a certainty that the number of occurrences will fall between the limits to±t∙.

Thus, suppose a shilling is tossed 200 times in succession; here P = 3 r = ⅛ and « = g = ⅛ · If therefore r = 30, it may be called a certainty that hea<fwill turn up more than 70 and less than 130 times.

In the same case suppose we wish to find the limits w±r such that it is an even chance that the number of heads shall fall between them, if the second term of (11) be neglected, we see from the table that rα=-^r= *48,. ∙. r-4'8; so that the probability that the number of heads shall fall between 95 and 105 is p 5 = '52 + - 7 1 7 - e ~i = '57 nearly, ivy τr rather more than an even chance.

11. Neglecting the second term of (11), we see that p r depends r solely on the value of ra, or that of; so that, if the number of trials N be increased, the value of r, to give the same probability, increases as the square root of N∙, thus, if in N trials it is practically certain (when rα = 3) that the number of occurrences lies between 2>N±r, then, if the number of trials be doubled, it will be certain that the occurrences will lie between 2jt>N±r√2.

In all cases, if N be given, r can be determined, so that there is a probability amounting to certainty that the ratio of the number of occurrences to the whole member of cases shall lie between the limits

∕>÷⅛∙

Now if N be increased roc √N; so that these limits are j, = t √5r

C being a constant. Hence it is always possible to increase the number of trials till it becomes a certainty that the proportion of occurrences of the event will differ from p, its probability on a single trial, by a quantity less than any assignable. This is the celebrated theorem given by James Bernoulli in the Ars Conjectandi. (See Todhunter’s History, p. 71.) [9:19:773]

12. We will give here a graphical representation (fig. 1), taken from Μ. Quetelet’s Lettres sur la Théorie des Probabilités, of the facilities of the different numbers of successes which may occur in 1000 trials as to any event which is equally likely to happen as not in each trial, —as in 1000 tosses of a coin, or 1000 drawings from an urn containing one white and one black ball, replacing the ball each time,—or again in drawing 1000 balls together from an urn containing a great number of black and white in equal proportion.

As p=q=⅛, we find from formula (8) that the chance of exactly half the entire number drawn, viz., 500, being white is

T= 1 . = 02523;

√500π

and the chance for any number 500 ±s is found by multiplying s¾

T by e δθo.

If then we take the central ordinate to represent T on any scale, and arrange along the horizontal line AB the different numbers of white balls which may occur, and erect opposite each number an ordinate representing the probability of that number, we have a graphical diagram of the relative possibilities of all possible proportions of black and white in the result.

We see from it that all values of the number of white balls drawn less than 450, or greater than 550, may be considered impossible, the probabilities for them being excessively small.

The probability of the number of white balls falling between any two assigned limits, as 490 and 520, is found by measuring the area of the figure comprised between the two ordinates opposite those numbers, and dividing the result by the total area.

II. Probability of Future Events Deduced from Experience.

13. In our ignorance of the causes which influence future events, the cases are rare in which we know a priori the chance, or “facility,” of the occurrence of any given event, as we do, for instance, that of a coin turning up head when tossed. In other cases we have to judge of the chances of it happening from experience alone. We could not say what is the chance that snow will fall in the month of March next from our knowledge of meteorology, but have to go back to the recorded facts. In walking down a certain street at 5 o’clock on three different days, I have twice met a certain individual, and wish to estimate from these data the likelihood of again meeting him under the same circumstances—in ignorance of the real state of things, viz., that he lives in that street, and returns from his business at that hour. Such is nearly the position in which we stand as to the probabilities of the future in the majority of cases.

We have to judge then, from certain recorded facts, of the probability of the causes which have occasioned them, and thence to deduce the probabilities of future events occurring under the operation of the same causes. The term “cause” is not here used in its metaphysical sense, but as simply equivalent to “antecedent state of things.”

Let us suppose two urns, A containing two white balls, B containing one white and one black ball, and that a person not knowing which is which has drawn a white ball from one, to find the probability that this is the urn A. This is in fact to find, supposing a great number of such drawings to be made, what proportion of them have come from the urn A. If a great number N of drawings are made indiscriminately from both urns, ⅜N come from the urn A and are all white, ∣ N white come from the urn B, and ∣N black. The drawing actually made is either one of the ⅜N white from A, or of the ∣N white from B. As it is equally likely to have been any one of these, the chance that it came from A is ⅜ N ÷ ∣ N, or ∣.

Suppose there had been two urns A and three ums B, and a white ball has been drawn from one of the five; as in a great number N of drawings ⅝N come from A and are white, ⅜N from B and ⅜ of them are white, the chance that it came from one of the urns A is

⅝÷(⅜+⅜4)-f∙

In general suppose an event to have occurred which must have been preceded by one of several causes, and let the antecedent probabilities of the causes be P1 ,P 2 ,P 8 . · ·

and let p 1 be the probability that when the first cause exists the event will follow, p 2 the same probability when the second cause exists, and so on, to find, after the event has occurred, the probabilities of the several causes or hypotheses.

Let a great number N of trials be made; out of these the number in which the first cause exists is P 1 N, and out of this number the cases in which the event follows are ∕> 1 P 1 N; in like manner the cases in which the second cause exists and the event follows are 2⅞P 2 N; and so on. As the event has happened, the actual case is one out of the number

( 7h p ι +Γ2 p 2 +^3 p 3+ &c · ) n .

and as the number in which the first cause was present is - p 1 P 1 N the a posteriori probability of that cause is πι ^P 1 +p 2 P 2 1 +k p 3 + ⅛c, · (12) ·

So likewise for the other causes,—the sum of these a posteriori probabilities being 1r 1 + π 2 + 1r 3 ÷ · · · · = 1 ∙

Supposing the event to have occurred as above, we now see how the probability as to the future, viz., whether the event will happen or fail in a fresh trial, is affected by it. If the first cause exists, the chance that it will happen is p 1 ; hence the chance of its happening from the first cause is ^ 1 1r 1 ; so likewise for the second, third, &c. Hence the probability of succeeding on a second trial is

Pι ιr ι+ Pi π ι+P3*3 + · · (13).

14. To give a simple example: suppose an um to contain three balls which are white or black; one is drawn and found to be white. It is replaced in the urn and a fresh drawing made; find the chance that the ball drawn is white. There are three hypotheses, which are taken to be equally probable a priori, viz., the urn contains three white, two white, or one white,—that of none white being now impossible. The probability after the event of the first is by (12)

⅜+H∏Γ* i that of the second is ⅛, that of the third ⅜.

Hence the chance of the new drawing giving a white ball is

⅜ + ∣∙⅛ + ⅛∙⅛ = l∙

15. The calculations required in the application of formulas (12) and (13) are often tedious, and such questions may often be solved in a simpler manner. Let us consider the following

An urn contains n black or white balls. A ball is drawn and replaced; if this has been done r times, and in every case a white ball has appeared, to find the chance that the (r + l)th drawing will give a white ball.

If s drawings are made successively from an um containing n balls, always replacing the ball drawn, the number of different ways this may be done is clearly n , .

If there be n +1 such urns, one with 0 white balls, one with 1 white, one with 2 white, &c., the last with n white, the whole number of ways in which r drawings can be made from any one of them is (n+l)n r .

Now the number of ways in which r drawings, all white, can be made from the first is 0, from the second 1, from the third 2 r , from the fourth 3 r , and so on; so that the whole number of ways in which r drawings of a white ball can be made from the n +1 urns is l r + 2 r + 3 r + . . . n r .

Hence the chance that if r drawings are made from an urn containing n black or white balls all shall be white is l r + 2 r + 3 r + . . . n τ ∙P r ~ (n + l)n r ’
[9:19:774]

for all we know of the contents of such an urn is that they are equally likely to be those of any one of the n + 1 urns above.

If now a great number N of trials of r drawings be made from such urns, the number of cases where all are white is p r N. If ?' +1 drawings are made, the number of cases where all are white is p r+1 N; that is, out of the ∕> r N cases where the first r drawings are white there are A·+iN where the (r + l)th is also’white; so that the probability sought for in the question is p r+ ι 1 l→ι + 2 r ÷ 1 + 8 r ÷ 1 + . . . n r + 1 . τ * p r n l r +2 r + 3 r + . . . n r

16. Let us consider the same question when the ball is not replaced. First suppose the n balls arranged in a row from À to B as below, the white on the left, the black on the right, the arrow marking the point of separation, which point is unknown (as it would be to a blind man), and is equally likely to be in any of its π + l possible positions.

A ι a B

OOOOOOOOOOO.

Î

Now if two balls, 1 and 2, are selected at random, the chance that both are white is the chance of the arrow falling in the division 2B of the row. But this chance is the same as that of a third ball 3 (different from 1 and 2), chosen at random, falling in 2B,— which chance is ⅜, because it is equally probable that 1, 2, or 3 shall be the last in order. It is easy to see that these chances are the same if we reflect that, the ball 3 being equally likely to fall in Al, 12, or 2B, the number of possible positions for the arrow in each division always exceeds by 1 the number of positions for 3; therefore as 3 is equally likely to fall in any of the three divisions, so is the arrow.

The chance that two balls drawn at random shall both be white is thus ⅜; in the same way that for three balls is ⅛, and so on. Hence the chance that r balls drawn shall all be white is

1^ r " ’ r+l ’the same chance for r +1 balls is

^ r+1 ~r+2 ’

thus, as in a large number N of trials the number of cases where the first r drawn are white is 7> r N, and the number where the first r +1 are white is 2⅛+ 1 N, we have the result :—

If r balls are drawn and all prove to be white, the chance that the next drawn shall also be white is

_2> r+ i_r+l ^ a p r r + 2'

This result is thus independent of n, the whole number of balls. This result applies to repeated trials as to any event, provided we have really no a priori knowledge as to the chance of success or failure on one trial, so that all values for this chance are equally likely before the trial or trials. Thus, if we see a stranger hit a mark four times running, the chance he does so again is ⅜; or, if a person, knowing nothing of the water where he is fishing, draws up a fish each time in four casts of his line, the same is the chance of his succeeding a fifth time. 1

In cases where we know, or rather think we know, the facility as to a single trial, if the result of a number of trials gives a large difference in the proportion of successes to failures from what we should anticipate, this will afford an appreciable presumption that our assumption as to the facility was erroneous, as indeed common sense indicates. If a coin turns up head twenty times running, we should say the two faces are probably not alike, or that it was not thrown fairly. We shall see later on, when we come to treat of the combination of separate probabilities as to the same event, the method of dealing with such cases (see art. 39).

We will give another example which may be easily solved by means of (12), or by the simpler process below.

There are n horses in a race, about which I have no knowledge except that one of the horses A is black; as to the result of the race I have only the information that a black horse has certainly won: to find the chance that this was A—supposing the proportion of black among racehorses in general to be p; i.e., the probability that any given horse is black is p.

Suppose a large number N of trials made as to such a case. A wins in —N of these. Another horse B wins in —N; out of these » n

B is black in —N^. Likewise for C; and so on. Hence the actual case which has occurred is one out of the number

1 . τ n-l. τ

—N -j N»;

71 71 ∙ r and, as of these the cases in which A wins are ^N, the required chance that A has won is

1l + (τi-l) j p ’

17. We now proceed to consider the important theorem of Bayes (see Todhunter, p. 294; Laplace, Théorie Analytique des Prob., chap. 6), the object of which is to deduce from the experience of a given number of trials, as to an event which must happen or fail on each trial, the information thus afforded as to the real facility of the event in any one trial, which facility is identical with the proportion of successes out of an infinite number of trials, were it possible to make them.

Thus we find in the Carlisle Table of Mortality that of 5642 persons aged thirty 1245 died before reaching fifty; it becomes then a question how far we can rely on the real facility of the event, that is, the proportion of mankind aged thirty who die before fifty not differing from the ratio ⅜⅜⅜⅞ by more than given limits of excess or defect. Again, it may be asked, if 5642 (or any other number of) fresh trials be made, what is the probability that the number of deaths shall not differ from 1245 by more than a given deviation?

The question is equivalent to the following :—An urn contains a very great number of black and white balls, the proportion of each being unknown; if, on drawing m+n balls, m are found white and n black, to find the probability that the proportion of the numbers in the urn of each colour lies between given limits.

The question will not be altered if we suppose all the balls ranged in a line AB (fig. 2), the white ones on the left, the black on the right, the point X where they meet being unknown and all positions for it in AB being a priori equally probable. Then, m + n points having been chosen at random in AB, m are found to fall on AX, n on XB. That is, all we know of X is that it is the (τn + l)th in order beginning from A of τn + τι + l points chosen at random in AB. If we put AB = 1, AX=ae, the number of cases when the point X falls on the element dx, is measured by lm+n , .---¾ m (l - x) n dx,

∣nt |?t '

since for a specified set of m points, out of the m + n, falling on AX, the measure would be z m (l - x) n dx, and the number of such

17Z⅛ - 4 - Tl∕

sets is L· i , . Now the whole number of cases is given by integrat-I r ∩l> Ί1/ ing this differential from 1 to 0; and the number in which X falls between given distances a, β from A is found by integrating from β to a. Hence the probability that the ratio of the white balls in the urn to the whole number lies between any two given limits α, β is

∕*^c"(l - x} κ dx d<)∙

∕ a; m (l - x} n dχ,

The curve of frequency for the point X after the event—that is, the ordinate of which at any point of AB is proportional to the frequency or density of the positions of X in the immediate vicinity of that point—is y=x w (l -«)"; the maximum ordinate KV occurs at a point K, dividing AB in the ratio τzt: n,— the ratio of the total numbers of white and black balls being thus more likely to be that of the numbers of each actually drawn than any other.

Let us suppose, for instance, that three white and two black have been drawn; to find the chance that the proportion of white balls is between f and ⅜ of the whole; that is, that it differs by less than ± ⅜ from ∣, its most natural value.

∕^ l ≈ ’ (l-≈)≈⅛t 225β ls z· ,., . ^-25 m,γ ⅛∙

∕ ae 3 (l -'x) 2 dx

18. An event has happened τn times and failed τι times in m + n trials. To find the probability that inp+ÿ further trials it shall happen p times and fail q times, —that is, that, p + q more points l It may be asked why the above reasoning does not apply to the case of the chance of a coin which has turned up head r times doing so once more. The reason is that the antecedent probabilities of the different hypotheses are not equal. Tims, let α shilling have turned up head once; to find the chance of its doing so a second time. In formula (12) three hypotheses may be made as to a double throw I’ two heads, 2° a head and tail, 3° two tails; but⅛he proba-bdιties of these are respectively ⅛, J, J; therefore by (12) the probability of the 1st after the event Is ⅛÷(i+l · ⅛)=}; that of the second is also 1; and by (13) the probability of succeeding on a second trial is 7 because, if hypothesis 2* is the true one, the second trial must fail.
[9:19:775]

being taken at random in AB, p shall fall in AX and q in XB. The whole number of cases is measured by lm + n ∕~ 1 ∖ m + n r x

(AB)* ’ +i i —r ∕ α w (l -x) n dx=~^~ ∕ z∙*(l -x} n dx.

' ' ∣ m ∣n√ 0 x ' ∖ m ∖ nj,

The number of favourable cases, when any particular set of p points, out of thejp + j additional trials, falls in AX, is measured ⅛ . 1 m+n r l . , - I α"+> ’ (l - o:) n +?ifø, I w KA

because, the number of cases as to the m + n points being, when X falls on the element dx, I m + n 1 —z β, (l - x) n dx, |?n ∖ n each of these affords ae*(l - ae)« cases where p now points fall on AX, and q on XB.

Now, the number of different sets of p points being ∖ P+<1

|p|? ’ the required probability is ta Λ*α-e⅛ , w .

⅛⅛- ∕^'if(l-x)∙⅛

J 0 or, by means of the known values of these definite integrals, ∣ jP + g ∣ m+p ∣ n + 9 ∣ m+n + l ^° f ~ ∖ p ∖ q - ~ ∣ ffl ∣jι ‘ ∣ ra÷¾+p+g+l ’

For instance, the chance that in one more trial the event shall happen is This is easy to verify, as the line AB has been divided into τn + n + 2 sections by the m+n+1 points taken on it (including X). Now if one more trial is made, i.e., one more point taken at random, it is equally likely to fall in any section; and m +1 sections are favourable.

19. When the number of trials m + n in art. 17 is large, the probability is considerable that the facility of the event on a single trial will not differ from its most natural value, viz., m by m + n more than a very small deviation. To make this apparent, we shall have to modify the formula (14), which gives for the chance that this facility lies between the limits a and β (by substituting for the denominator its known value), lm + π + l ∕∙β , _ p =—, , —∕ xi m (l-χ} n dx .... (17).

I ∖m ∖nja

To find now the probability that the facility lies between the limits 3= m + δ, and α=—— δ, where δ is small. Put for m+n m+n

972»

%, + x; and (17) becomes ’ m+n ’ v ’

∣m + n + l ∕ ∕ m ∖ mf n ∖ n , p≈ - l ∣ = ∕ ( 1- X ) ( ; X ) dx . ∖-∖- κ ^~ δ ∖ wi + m J ∖ m + n J

Now if X is small, and we put u=(a+x) m , SC logu = mloga+m- - mx mx 2 .∙. u=a m e~H^ 2α 2 correct as far as the square of x. Hence the two factors under the sign of integration become (m + n)" e 2m ’· and {m+n)- e 2n so that

∣m + n + l rn m n n Γ (m+nf „ y - ∣ g ⅛, ( w ÷n).n÷√√^^~^ < 18 >∙ Now, since by Stirling’s theorem ∣wt=m m +⅛- m √2w, the constant coefficient here becomes (m+n + Y)(m+n) m + n +le ~ m ~ n ^f2π m m n n _(m + rift m m .n n .e~ m ~ n 2τ ∖ Jmn (τn + n) m + n √2τnnιr taking m + n + l = m+n. Now if we substitute in (18) i -√⅛≤! (19), ∖ l2mn p=∙-je~ t *dt, where λ = δ⅛⅛^- (20),

√2wm

2 ∕' λ .∕2

or finally - p ~√⅛ k Λ e dt < 21 )»

for the approximate value of the probability that the real facility of the event lies between the limits —- ⅛δ.

772 + 72-

Thus, if out of 10,000 trials, the event has happened 5000 times, the probability that, out of an infinite number, the number of successes shall lie between ⅜±- 2 ¼, or between ⅜⅜⅜ and √⅛⅞, of the whole, will be p= ’ 678 = f nearly, 10 6 for we find from (20) λ ° ∣ 10 4, s y 2 · τo= '7 nearly; and, referring to the table in art. 9, we find the above value for the integral (21).

We must refer to the sixth chapter of Laplace for the investigation of how far the number of successes in a given number of fresh trials may be expected to deviate from the natural proportion, viz., that of the observed cases—as also for several closely allied questions, with important applications to statistics.

III. On Expectation.

20. The value of a given chance of obtaining a given sum of money is the chance multiplied by that sum; for in a great number of trials this would give the sum actually realized. The same may be said as to loss. Thus if it is 2 to 1 that a horse will win a race, it is considered a fair wager to lay £10 to £20 on the result; for the value of the expected gain is j of 10, and that of the expected loss ⅜ of 20, which are equal. Thus, if the probabilities for and against an event are p, q, and I arrange in any way to gain a sum a if it happens and lose a sum b if it fails, then if pa≈qb I shall neither gain-nor lose in the long run; but if the ratio a : b be less than this, my expectation of loss exceeds that of gain; or, in other words, I must lose in the long run.

The above definition is what is called the mathematical expecta tion ; but it clearly is not a proper measure of the advantage or loss to the individual; for a poor man would undoubtedly prefer £500 down to the chance of £1000 if a certain coin turns up head. The importance of a sum of money to an individual, or its moral value, as it has been called, depends on many circumstances which it is impossible to take into account; but, roughly and generally, there is no doubt that Daniel Bernoulli’s hypothesis, viz., that this importance is measured by the sum divided by the fortune of the individual ^[5. This rule must be understood to hold only when the sum is very small, or rather infinitesimal, strictly speaking. It would lead to absurdities if it were used for large increments (though Buffon has done so; see Todhunter, p. 345). Thus, to a man possessing £100, it is of the same importance to receive a gift of £100 as two separate gifts of £50; but this rule would give as the measure of the importance of the first ⅜gg=l; while in the other case, it would give √⅞‰+ ,^⅛=g. The real measure of the importance of an increment when not small is a matter for calculation, as shown in the text.] —is a true and natural one. Thus, generally speaking, £5 is the same to a man with £1000 as £50 to one with £10,000; and it may be observed that this principle is very generally acted on, in taxation, &c.

21. To estimate, according to this hypothesis, the advantage or moral value of his whole fortune to the individual, or his moral fortune, as Laplace calls it, in contradistinction to his physical fortune, let x = his physical fortune, y=his moral fortune, then, if the former receive an increment dx, we have, from Daniel Bernoulli’s principle, , 7 dx d,J-k-, -∙-V-klo i i (22), lb k, h being two constants, x and y are always positive, and x>h ; for every man must possess some fortune, or its equivalent, in order to live.

22. To estimate now the value of a moral expectation. Suppose a person whose fortune is a to have the chance p of obtaining a sum a, q of obtaining β, r of obtaining γ, &c., and let p + q+r+ . . . =1, only one of the events being possible. Now his moral expectation from the first chance—that is, the increment of his moral fortune into the chance—is pk ∖ log^^-log^ [ =pklog(a + d)-pkloga, ( lb Γb J

Hence his whole moral expectation is^[6. important to remark that we should be wrong in thus adding the expectations if the events were not mutually exclusive. For the mathematical expectations it is not so.] Έ =kp log (a + a) + kqlog (a+β) + kr log (a+ y) + . . . -⅞logα; [9:19:776] and, if Y stands for his moral fortune including this expectation, that is, k log -y + E, we have

Y=⅛log(α + α) + ⅛ g(a + j8)+ . . . -ArlogΛ. . (23). Let X be the physical fortune corresponding to this moral one, by (22)

Y ≈k log X - k log h.

Hence X — (α + α)> ’ (α + β) , (a + γ) r (24); and X— a will be the actual or physical increase of fortune which is of the same value to him as his expectation, and which he may reasonably accept in lieu of it.

The mathematical value of the same expectation is pa+qβ + ry + (25).

23. Several results follow from (24). Thus, if the sums a, β, y . . . are very small, it is easy to see that the moral expectation coincides with the mathematical, for

X-α→÷∙÷∙∙ (1+ <∕(l + f)∙. · · -^ +, (l+⅛ ÷if + ∙ · )· . ’ . X=a+pa + qβ + . . .

24. We may show also that it is disadvantageous to play at even a fair game of chance (unless the stakes are very small, in which case the last article applies). Thus, suppose a man whose fortune is a plays at a game wmere his chance of winning a sum a is p, and his chance of losing a sum β is q — 1 -p. If the game is fair, pα=gj3.

Now by (24) the physical fortune which is equivalent to his prospects after the game is

X = (α + α)J ’ (α - j3)f, β g or X=(a + a) a +0(a-0) a +0.

Now the geometrical mean of r quantities is less than the arithmetical,^[7. A very simple proof of this principle Is as follows let a number N be divided Into r parts a, b, c, &c.; if any two of these, as a, b, are unequal, since m ’ >-∙ it follows that the product abed .. . is increased by substituting , for a and b. Hence as long as any two are unequal we can divide N τ differently so as to obtain a greater product; and therefore when the parts are all eαual the product Is greatest, or ∕a+δ⅛e+ . . .∖r^ ( F )>aδc...] so that if there are β quantities α + α, and α quantities a-β, j (.+«)’(« - β)∙ i ’ ~ 3 <⅜⅛⅛-⅜,

( J a+β ’ or X<α, so that he must expect morally to lose by the game.

25. The advantage of insurance against risks may be seen by the following instance. A merchant, whose fortune is represented by 1, will realize a sum e if a certain vessel arrives safely. Let the probability of this be p. To make up exactly for the risk run by the insurance company, he should pay them a sum

(l-p)e.

If he does, his moral fortune becomes by (22) k↑og 1 ÷jp; II. while, if he does not insure, it will be (23).

⅛logl±J .

Now the first of these exceeds the second, so that he g ains by insuring on these terms; because log(l+j⅛) >plog(l + t), i_

that is (l+p*) p >l + e; , ... m for, putting »— , r m + n

(«-)-> (1+<r , because (see note art. 24), if m (1 + e) + n is divided into m + n equal parts, their product is greater than that of m parts each equal to 1 + e and n parts each equal to 1.

The merchant will still gain by paying, over and above what covers the risk of the company, a sum α, at most, which satisfies log(l -α+p*)-ψlog(l + e)j .’. a=l+2¼-(l + e)> ’.

By paying any sum not exceeding this value, he still gains, while the insurance office also makes a profit, which is really a certainty when it has a large business; so that, as Laplace remarks, this example explains how such an office renders a real service to the public, while making a profit for itself. In this it differs from a gambling establishment, in which case the public must lose, in any sense of the term.

It may be shown that it is better to expose one’s fortune in separate sums to risks independent of each other than to expose the whole to the same danger.^[8. The familiar expression not to “put all one’s eggs in the same basket” shows us how general common sense has recognized this principle.] Suppose a merchant, having a fortune a, has besides a sum e which he must receive if a ship arrives in safety. By (24) the value in money of his present fortune is

X = (α + e)> ’ αi, where p = chance of the ship arriving, and7 = l -p.

Now suppose he risks the same sum in two equal portions, in two ships. We cannot apply (23), as the events are not mutually exclusive; but we see that, if both ships arrive, the chance of this being p 2 , he realizes the whole sum e; if one only arrives, the chance being 2ρq, he receives ⅜e; if both are lost, the chance being q 2 , he loses all. Thus (24) he is now worth a sum

X' = (α + e )P 2 (α + ⅛ e )⅛⅛Λ

Now this sum is greater than the former; for

(α + f )P i ~P. (a + ⅜e) 2 ^. a? 2 "« > 1, that is, ( α + e ∙)- pq (a + ⅛γ pq a~P q > 1; for (α±i*)≡> 1

α(α + e ) as is obviously true.

Now suppose he risks the sum e in three separate ventures. His fortune will be

X" = (α + e )? 3 . (α + 2 e)⅛⅛. (α + ⅜e)⅛iV ’; and we have to show that this is worth more than when there were two. If we put a outside each bracket, and put δ = we have to prove

(1 + 3δ) p3 (l + 2δ) 3 ^. (1 + δ) 3 M 2 > (1 + 3δ)∙P 2 . (1 + ⅜δ)⅛i; or (1 + 3δ)* 2 "Λ (1 + 2δ) 3 ^(l + δ) 3 « 2 >(1 + ⅛δ) 2 *; or, since p 2 - p = -pq,

(1 + 3δ)~ p (l + 2δ)⅛(l + δ) 3 * > (1 + <δ) 2 ,

° t j (i÷⅜(i 8 ÷ 8 )∙ j ι,tl ÷ 8 > ,ι ∙< 1 ÷ ι8 > ii now (l + 2δ) 3 <(l + δ) 3 (l + 3δ)j hence the fraction in the brackets is always less than its pth power as p < 1; and λve can now show that (1÷ 1 3W÷√ 1 ÷ 8 ) ’ >< 1 + W i '

that is, (1 + 2δ) 3 > (1 + 3δ)(l + ⅜δ) 2 , or l + 6δ + 12δ 2 + 8δ 3 >l + 6δ + ⅛δ 2 + ^δ 3 ∙

Laplace shows (ch. x.) that the gain continues to increase by subdivision of the risk; it could no doubt be shown by ordinary algebra. He shows further that the moral advantage tends to become equal to the mathematical. This may be done more easily thus :—

The expression is, when e is divided into r equal parts, x- ( . + .f(« + .-;f-∖. + .4)V , --^....

and we have to find the limit towards which this tends as r becomes infinitely great.

Put z = α + ej

„ n √ e∖rP r ^ 1 5∕ . ∖⅛Lzl√ -2 i 2 ∕ t ∖ r<f^' l P∕ ∖ <t r

X- a "(≈--) (,- 2 i) s '...(s -(,-l)i) (i -.),

Now in the binomial expansion r r -1 r(r-l) r— 2 2 , l=p r + rp q+ v 1 . 2 , p q +. the greatest term is the (⅛r+l)th, viz.,

_ r(r-l). . . (rp + l) rp r 9 1 , 2 ’ 3 . . . rq p "

The factor in X corresponding to this is if we put U=2 - qt.

Let us now express the binomial series before and after T thus:

1= . . . +T3 + T2 + T 1 + T + i 1 + i2 + ⅛÷ · · ·
[9:19:777]

and we have

X-. . . (u+2i) τ, (u + i)V(ul)' ’ (u-21)' i . . .

The factors towards the beginning and end may be all taken as 1, because the terms of the binomial increase rapidly in value from either end when r= <×>, and we shall have the true limit for X by taking an indefinitely great number of factors on either side of U, which number, however, may be infinitely less than r.

As the sth factors before and after U may be expressed thus (s being always very small compared to r)—

(ue⅛) τ, > (ue~ i ⅛) ’ ''

and as . . . T 2 + T 1 + T + i 1 + ⅛ . . . =1, we have χ=U⅛<∙∙∙ 2τ ≡+ τ -⅛-∙∙∙∖

Now we have seen in art. 8 that i 2 n-y s t p-q.

T, = Tβ 2pρτ + 2pjr , t, = Te 2pqr 2pqr

Hence T i -i l =Tβ-⅛^=⅛ 5

.∙.sT t -⅛≈¾ s3 e - 2 ⅛.

pq r pι

Hence the exponent of e above becomes * p-q t v s =*t* 2 jL rU ’ pq ' ½ j s =0 r ⅛ iqr ’ s 1 being the extreme limit for s. s

If we put ® = and a⅛ - ⅛=⅛), the above sum is s 1 ≠(°)+≠(⅛)+≠(⅛)+ · · · ≠(⅛)= ri j ∕f·

Now it is easy to prove that

∕∙α°

∕ x 1 e ^κ,dx is finite:

and much more is it so λvhen the superior limit is finite.

Hence the exponent of e becomes

· ri ∙ κ = W · — · TIG∙-i, rU pq U pq where K is finite; so that the exponent becomes infinitesimal when r=∞.

The limit therefore towards which X tends is

X = U = z - qe = a +pe, that is, the mathematical value of the fortune.

The very important applications of probability to annuities and insurance are to be found in the articles on those subjects, to which therefore we refer the reader.

IV. Probability of Testimony.

26. We have here to treat of the probability of events attested by several witnesses of known credibility, or which have several different probabilities in their favour, derived from different independent sources of information of any kind, of known values.^[9. The question now before us is quite different from that of the chance of an event happening or having happened whiclι may happen in different ways, in which case we add the separate probabilities. Tims if there are but two horses in a race, of equal merit and belonging to one owner, his chance of winning is i+}=l. But suppose I only know that one of the two is his, and, besides, some one whose credibility is ∣ tells me he has won the race; here I have two separate probabilities of i each for the same event; but it would clearly be wrong to add them together.]

A witness may fail in two ways: he may be intentionally dishonest, or he may be mistaken; his evidence may be false, either because he wishes to deceive, or because he is deceived himself. However, we will not here take separate account of these two sources of error, but simply consider the probability of the truth of a statement made by a witness, which will be a true measure of the value of his evidence. To estimate this probability in any given case is not an easy matter; but if we could examine a large number of statements made by a certain person, and find in how many of them he was right, the ratio of these numbers would give the probability that any statement of his, taken at random, whether past or future, is a true one.

27. Suppose a witness, whose credibility is p, states that a fact occurred or did not occur, or that an event turned out in one way, when only two ways are possible. If nothing was known a priori as to the probability of the fact, or if its real facility was ⅜, it is clear that the probability that it did occur is p. For if a great number N of trials were made (either really as to the event, if its facility is known to be ⅜, as in tossing a coin; or as to it and other cases resembling it as to our ignorance of the real facility, if such is the state of things) in ⅜N the event happens, and out of these the witness asserts in ⅛N cases that it did happen. Now, out of the whole number, he asserts in ⅜N cases that it happened, as there is no reason for his affirming offener than he denies (or, it may be said, he affirms in ⅜pN cases where it did happen, and in ⅛(1 -p)N cases where it did not). Hence, dividing the whole number of cases when it happens and he affirms it by the whole number of cases where he affirms it, we find ⅜pN÷⅜N =p.

We have entered at length on the proof of what is almost self-evident (perhaps indeed included in the definition) in this case, because the same method will succeed in other cases which are not so easily to be discerned.

28. Let us now consider the same question when the a priori probability of the fact or event is known. Suppose a bag contains n balls, one white and the rest black, and the same witness says he has seen the white ball drawn; what is the chance that it was drawn?

A great number N of trials being made, the number in which the white ball is drawn is — N, and out of these he states it in n ’ n^⅛N cases. Out of the remaining (l-n^ 1 ) N cases where a black ball was drawn, he says (untruly) that in (1 -p) (1 -n^ 1 ) N cases it was white.

Now, dividing the number of favourable cases, viz., those where he says it is white and it is so, by the whole number of cases, viz., those where he says it is white, we have for the probability required w = ⅛ _ p (26).

π^ 1 p + (l-n^ 1 )(l- j p) n-l-(n-2)p ' 7

This holds for any event whose « priori probability is n^ 1 .

If n be very large, this probability will be very small, unless p is nearly =1; and, indeed, if we go back to the common sense view, it is clear we should hesitate to believe a man who said he had drawn the white ball from a bag containing 10,000 balls, all but it being black. It may be observed that if n=2, -≈r=p, as in art. 27.

We have thus a scientific explanation of the universal tendency rather to reject the evidence of a witness than to accept the truth of a fact attested by him, when it is in itself of an extraordinary or very improbable nature.

29. Two independent witnesses, A and B, both state a fact, or that an event-turned out in a particular way (only two ways being possible), to find the probability of the truth of the statement.

Supposing nothing is known a priori as to the event in question, let a great number N of trials be made as to such events; the number of successes will be ⅜N; out of these the witness A affirms the success in ⅜pN cases; out of these the witness B affirms it, too, in ⅜pτ>'N cases.^[10. Here we are assuming the independence of the witnesses. If B, for instance, were disposed to follow A , s statements or to dissent from them, he would affirm the success here in more or less than App'N cases.] Out of the ⅜N failures A affirms a success in ⅜(1 -p)N cases; and out of these B also affirms one in ⅜(1 - 7 1 )(1 -∕)N cases. Hence, dividing the favourable cases by the whole number, the probability sought is W= BP' + (1 -Γ)(l-∕) ^ 2z ^ ’ where p, p' are the credibilities of the two witnesses.

This very important result also holds if p be the probability of the event derived from any source, and p' the credibility of one witness, as in art. 28; or if p and p' be any independent probabilities, derived from any sources, as to one event.

30. We give another method of establishing the formula (27). Referring to art. 13, the observed event is the concurrent evidence of A and B that a statement is true. There are two hypotheses —that it is true or false. Antecedent to B’s evidence the probabilities of these hypotheses are p and 1 -p (art. 27), as A has said that it is true. The observed event now is that B says the same. On the first hypothesis, the probability that he will say this is p' ; on the second, it is 1 -p'. Hence by formula (12) the probability a posteriori of the first hypothesis, viz., that the joint statement is true, is, as before, PP' pp' + (l-^)(l-p') '

31. If a third witness, whose credibility is p", concurs with the two former, we shall have to combine p" with w in formula (27); hence the probability &' of the statement when made by three witnesses is , -≈p'' pp'p" Z 9fib

^ a °wp"+(l-≡r)(l-y ’) pp⅛" + (l-p)(l-y)(l-p") l ’ and so on for anv number.
[9:19:778]

As an example, let us find how many witnesses to a fact, the odds against which are 1,000,000,000,000 to 1, would be required to make it an even chance that the fact did occur, supposing the credibility of each witness to be p≈τo∙

Let X be the number.

10- 12 jtr c P z i ~ 10- 12 p* + (1 -10 - 12 )(1 - p) x ~p x + 10 12 (l -pY ’

• 2-l + ¾ ’ · 9*

.∙∙ x- r ¾-12∙6jlog 9

so that thirteen such witnesses would render the chance more than an evejι one.

32. Let us now consider an event which may turn out in more than two ways, and let each way be equally probable a priori, and suppose a witness whose credibility is p states that it turned out in a certain way; what is the chance that it did so?

Thus if a die has been thrown, and he states that ace turned up; or if tickets in a lottery are numbered 1, 2, 3, &c., and he states that 1 was drawn; to find the chance that he is right.

Take the case of the die, and suppose a great number N of throws. In ⅜N the ace turns up, and he says so in ⅛N cases. In ⅛N the two turns up, and he is wrong in ⅛(1 -,p)N cases out of these; but he says ace in only ⅜ of these, as there is no reason why he should give it more or less often than any of the five wrong numbers. In the same way for the other throws; so that the whole number of cases where he says ace turned up is

⅛N + ⅜.∣(l- i )N=⅜N5

and, the number, out of these, when it actually turned up being ⅜T>N, we find the chance it did turn up is p, the credibility of the witness. In any such case, this result will hold. We might indeed safely have argued that when the die is thrown a great number of times, any witness, whatever his veracity, will quote each face as often as any other, as there is no reason for one to turn up oftener than another, nor for him to affirm, rightly or wrongly, one rather than another; so that he will say ace in ⅜N of the throws, while he says ace in ⅛N out of the ⅜N cases where it does turn up.

This result compared with art. 28 affords an apparent paradox. If a large number of tickets are marked 1,0,0,0,0,0 . . . . and a witness states that 1 has been drawn from the bag, we see from art. 28 that the chance he is right is very small; whereas if the tickets were marked 1,2,3,4,5,6 .... and he states that 1 has been drawn, the chance he is right is p, his own credibility. However, we must remember that in the first case he is limited to two statements, 1 and,0, and he makes the first, which is very improbable in itself; whereas in the other case, the assertion he makes is in itself as probable as any other he can make— e.g., that 2 was the ticket drawn—and therefore our expectation of its truth depends on his own credibility only.

33. Suppose now that two witnesses A, B both assert that the event has turned out in a certain way,—there being, as in art. 32, n equally probable ways.

Both, for instance, say that in a lottery numbered 1,2,3,4,5 .... No. 1 has been drawn. A large number N of drawings being made, 1 is drawn inn -1 N cases; out of these A says 1 in n^⅛N cases, and out of these B also says 1 in n- r pp'N. No. 2 is drawn in n^ 1 N cases; here A is wrong in n^ 1 (l - jj)N, but says 1 in only (n-l)- 1 ∕i^ 1 (l-p)N; and B will also say 1 in (l-y)(n-l)^ 1 of these; that is, both agree that 1 has been drawn in

(n-l)^ 2 n -1 (l-p)(l-p')N

cases. So likewise if No. 3 has been drawn, and so on; hence, when No. 1 has not been drawn, they both say that it has in n* 1 (n-l)-i(l-p)(l-p')N

cases. Hence the number of cases where they are right divided by the whole number of cases where they make the statement, that is, the probability that No. 1 has been drawn, is w ^2>p' + (n-1)- 1 (1 -p)(l -p') · · · · ( 29 )∙

If n be a large number the chance that they have named the ticket drawn is nearly certainty. Thus, if two independent witnesses both select the same man out of a large number, as the one they have seen commit a crime, the presumption is very strong against him. Of course, for the case to come under the above formula, it is supposed that some one of the number must be guilty.

34. In the same case, when the event may turn out in n ways not equally probable, as in a race between n horses A, B, C . . . . whose chances of winning are a, b, c . . . ., so that a+b+c+ . . . =1, if one witness whose credibility is p states that A has won, it is easily shown by the same reasoning as in art. 33 that the probability A has really won is ap w== ap + (l-α)(n-1)^ 1 (1-p) ’ ’ ’ , (30) ’ and if two witnesses say so, it is a= a PP' ,< ξn

αjjp'+ (1-α)(n-1)^ 2 (1-p)(l-j∕) * * ’ V n '*

It is easily shown in formula (30) that if p>n~ 1 the probability w is increased by the testimony, beyond a, its antecedent value. Thus, suppose there are ten horses in a race, and that one of them, A, has a chance ⅛ of winning, and that just after the race I learn that a black horse has won, black being A’s colour; now, if I know that ⅜ of racehorses in general are black, this gives me a new chance ⅛ (see art. 16) that A has won. Therefore from (30) the chance of the event is now τ ⅛ = f.

35. To illustrate the effect of discordant testimony. In art. 29 let A have asserted that the fact occurred, and let B deny it. It is easy to see that 1 -p' is to be put for p', so that the probability that it did occur is

_ ff(l -p') . p(i-y)+yα-p) ( ' ’ if there had been an a priori probability a in favour of the fact this would have been

≡r= ~∙P ’ ) ∕o<n

αχi-√)+p'(l-α)(l-^) · · · · W

Thus if the credit of both witnesses were the same, p=p', and we find from (33) w=α, so that the evidence has not altered the likelihood of the event.

36. Where the event may turn out in n equally probable ways as in art. 33, and the witness A asserts one to have occurred, say the ticket marked 1 to have been drawn, while the witness B asserts another, say the ticket marked 2; to find the chance that No. 1 was drawn.

By the same reasoning as in art. 33 we find for the chance m P^~P'} ∕o 4 ∖

χi-∕) + (n-l)- 1 (l-p)(π-2+√) · ‘ ' k h

This result will also follow if we consider B’s evidence as testimony in favour of No. 1 of the value (1 -j∕)(n -1 )^ 1 .

When the number of tickets n is very great, (34) gives p-pp' Ή — -; .

37. As remarked in art. 26, the methods we have given for determining the probability of testimony apply to cases where the evidence is derived from other sources. Thus, suppose it has been found that a certain symptom (A) indicates the presence of a certain disease in three cases out of four, there is a probability ∣ that any patient exhibiting the symptom has the disease. This, however, must be considered in conjunction with the a priori probability of the presence of the disease, if we wish to know the value of the evidence deduced from the symptom being observed. For instance, if we knew that f of the whole population had the disease, the evidence would have no value, and the credibility of the symptom per se would be ⅜, telling us nothing either way. For if a be the a priori probability, tτ that after the evidence, p the credibility of the evidence, we have found ap v ~ αp + (l-α)(l-p) ’

so that, if w = α, ρ=⅜.

If w and a are given, the credibility p of the evidence is deduced from this equation, viz.,

(l-α)w ” a + w — 2aw ’

38. Suppose now the probability of the disease when the symptom A occurs is w (that is, it is observed that the disease exists in σN cases out of a large number N where the symptom is found), and likewise the same probability when another independent symptom B occurs is zr'. What is the probability of the disease where both symptoms occur?

Let a be the a priori probability of the disease in all the cases; then the value of the evidence of B is, as explained above, y-¾⅞* z -> a + ·or -2αw and this has to be combined with w, which is the probability of the disease after A is observed. We find the probability (∏) required to be

Π» .

wp' + (l-w)(l-p) ’
[9:19:779]
